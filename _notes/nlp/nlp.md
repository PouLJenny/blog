# NLP



## Java开源工具

[stanfordnlp](https://stanfordnlp.github.io/CoreNLP/)
[apache open nlp](https://opennlp.apache.org/)
[hanlp](https://hanlp.hankcs.com/)

### Stanford NLP



### HanLP

比较适合中文处理,其他语言不是很好


## 词嵌入 

大部分模型方法期望的输入是固定长度的数值特征向量,而不是不同长度的文本文件。所以需要词嵌入。  
词嵌入（Word Embeddings）是一种将词语或文本表示为固定维度的向量的方法。这些向量能够捕捉词语之间的语义关系，是自然语言处理（NLP）中的一种关键技术。词嵌入将词语映射到一个连续的向量空间中，相似语义的词语在该空间中彼此距离较近。


常用的算法：

### Word2Vec
Word2Vec方法使用的是distributed representation 的词向量表示方式。distributed representation 最早由Hinton在1986年提出，其基本思想是通过训练将每个词映射成$K$维实数向量（$K$一般为模型中的超参数）,通过词之间的距离(比如余弦相似度、欧式距离等)来判断它们之间的语义相似度。它采用一个三层的神经网络，即输入层-隐藏层-输出层，核心技术是根据词频使用Huffman编码，使得所有词频相似的词隐藏层激活的内容基本一致，出现频率越高的词语激活的隐藏层数目越少，这样有效地降低了计算的复杂度。Word2Vec输出的词向量可用于很多自然语言处理相关的工作，如聚类、分类、找同义词、词性分析等。

### 哈希表
哈希表算法通过哈希表用哈希函数来确定词块在特征向量的索引位置，可以不创建词典，称为哈希技巧。哈希技巧是无固定状态的，它把任意数据块映射到固定数目的位置，并且保证相同的输入一定产生相同的输出，不同的输入尽可能产生不同的输出。

### TF-IDF
TF-IDF文本特征提取算法的主要思想是：如果某个词或短语在一篇文章中出现的频率高，并且在其他文章中很少出现，则认为此词或短语具有很好的类别区分能力，适合用来分类。TF-IDF实际上是TF*IDF，即词频(term frequency)与反文件频率(inverse document frequency)。TF表示词条在文档$d$中出现的频率。IDF的主要思想是：如果包含词条$t$的文档少，也就是$n$越小，IDF越大，则说明词条$t$具有很好的类别区分能力。如果某一类文档$C$中包含词条$t$文档数为$m$，而其他类包含$t$的文档总数为$k$,显然所有包含$t$的文档数$n=m+k$,当$m$大的时候，$n$也大，按照IDF公式得到的IDF值会小，就说明该词条$t$类别区分能力不强。但是实际上如果一个词条在一个类的文档中频繁出现，则说明该词条能狗很好地代表这个类的文本的特征，这样的词条应赋予较高的权重，并选来作为该类文本的特征词以区别于其他类文档。


### 参考文档
[网络博客-白话词嵌入：从计数向量到Word2Vec](https://cloud.tencent.com/developer/article/1508923)




