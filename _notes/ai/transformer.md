# Transformer




## è®ºæ–‡

ä»¥ä¸‹æ˜¯å…³äº **Self-Attention ä¸ Transformer** çš„æ ¸å¿ƒè®ºæ–‡åŠå…¶è®¿é—®åœ°å€ï¼Œæ¶µç›–åŸå§‹ç†è®ºã€æ•ˆç‡æå‡ä¸è§£é‡Šæ€§ç ”ç©¶ï¼š

---

## 1. Transformer åŸºçŸ³

* **Attentionâ€¯Isâ€¯Allâ€¯Youâ€¯Need**ï¼ˆVaswani ç­‰ï¼Œ2017ï¼‰
  æå‡º Transformer æ¶æ„ï¼Œé¦–æ¬¡ä½¿ç”¨ self-attention å®Œå…¨æ›¿ä»£ RNN/CNNã€‚åŸæ–‡ PDF å¯è¯»äº NeurIPS èµ„æºåº“ã€‚
  ğŸ‘‰ arXiv/NeurIPS PDF (paper) ([arxiv.org][1], [papers.neurips.cc][2])
* ä¸­æ–‡ç»´åŸº/ç¿»è¯‘ä»‹ç»ä¸è§£è¯»æ–‡ç« ï¼š

  * ä¸­æ–‡ Wikiâ€œæ³¨æ„åŠ›å°±æ˜¯ä½ æ‰€éœ€è¦çš„ä¸€åˆ‡â€ ([zh.wikipedia.org][3])
  * è‹±æ–‡ Wiki æ¦‚è§ˆ Transformer æ¶æ„ ([blog.csdn.net][4])

---

## 2. è§†è§‰åŒ–ä¸è§£è¯»èµ„æº

* **The Illustrated Transformer / Selfâ€‘Attention**
  è¾ƒç›´è§‚å±•ç¤º Transformer æ¶æ„ä¸ selfâ€‘attention æœºåˆ¶ï¼Œæ·±å—ç¤¾åŒºæ¨å´‡ã€‚å¤šä½ç½‘å‹æ¨è â€œIllustrated: Self-Attentionâ€ ([reddit.com][5])
* å„ç±»ä¸­æ–‡è¯¦ç»†è§£è¯»ä¸åšæ–‡ï¼š

  * CSDN ä¸ çŸ¥ä¹ ç­‰å¹³å°è¯¦è§£ Transformer ç»“æ„ä¸ selfâ€‘attention ([blog.csdn.net][4])

---

## 3. å‡æ•ˆä¸æ‰©å±•æ–¹å‘ç ”ç©¶

* **Linformer: Self-Attention with Linear Complexity**ï¼ˆWang ç­‰ï¼Œ2020ï¼‰
  æå‡ºé€šè¿‡ä½ç§©è¿‘ä¼¼ï¼Œå°† self-attention çš„å¤æ‚åº¦ç”± $O(n^2)$ é™ä¸º $O(n)$ã€‚å¾ˆé€‚åˆå¤„ç†é•¿åºåˆ— ([arxiv.org][6])
* **NystrÃ¶mformer**ï¼ˆXiong ç­‰ï¼Œ2021ï¼‰
  åŸºäº NystrÃ¶m æ–¹æ³•æ¨è¿›è¿‘ä¼¼ self-attentionï¼ŒåŒæ ·è¾¾æˆçº¿æ€§æ—¶é—´å¤æ‚åº¦ï¼Œé€‚åº”é•¿æ–‡æœ¬ ([arxiv.org][7])
* **Theoretical Limitations of Self-Attention**ï¼ˆHahnï¼Œ2019ï¼‰
  æ¢è®¨ self-attention åœ¨è¡¨è¾¾å½¢å¼è¯­è¨€ç»“æ„ä¸Šçš„ç†è®ºè¾¹ç•Œï¼Œä¾‹å¦‚å…¶å¯¹å±‚æ•°/head æ•°çš„ä¾èµ– ([arxiv.org][8])

---

## 4. å¯è§£é‡Šä¸å› æœè§†è§’

* **Causal Interpretation of Self-Attention in Preâ€‘Trained Transformers**ï¼ˆ2023ï¼‰
  ä»ç»“æ„æ–¹ç¨‹æ¨¡å‹ï¼ˆSEMï¼‰çš„è§’åº¦è§£æ pre-trained transformers ä¸­ self-attention çš„å› æœæ„ä¹‰ ([arxiv.org][9])

---

## ğŸ“š é™„åŠ èƒŒæ™¯é˜…è¯»

* **Wired é•¿æ–‡** èåˆäº† Transformer å›¢é˜Ÿåœ¨ Google æˆç«‹å’Œ self-attention æ¦‚å¿µèµ·æºçš„ç²¾å½©æè¿° ([wired.com][10])
* **TechRadar ç§‘æ™®æ–‡ç« ** æ€»è§ˆäº† Transformer åœ¨ AI ä¸­å¦‚ä½•è„±é¢–è€Œå‡º ([techradar.com][11])

---

## ğŸ“¥ å¿«é€Ÿæ±‡æ€»è¡¨

| ä¸»é¢˜æ–¹å‘           | è®ºæ–‡ / æ–‡æ¡£                       | ç®€è¦è¯´æ˜                 |
| -------------- | ----------------------------- | -------------------- |
| æ ¸å¿ƒæ¶æ„           | â€¯Attentionâ€¯Isâ€¯Allâ€¯Youâ€¯Need    | Transformer æºèµ·       |
| è§£è¯»èµ„æ–™           | The Illustrated Transformer ç­‰ | å¯è§†åŒ–ç†è§£ selfâ€‘attention |
| é«˜æ•ˆ Transformer | â€¯Linformer,â€¯NystrÃ¶mformer     | é™ä½è®¡ç®—å¤æ‚åº¦              |
| ç†è®ºåˆ†æ           | Theoretical Limitations       | æ·±å…¥ç†è§£è¡¨è¾¾è¾¹ç•Œ             |
| å¯è§£é‡Šæ€§ç ”ç©¶         | Causal Interpretation ...     | æä¾›å› æœè§†è§’               |
| èƒŒæ™¯ç§‘æ™®           | Wired, TechRadar æ–‡æ‘˜           | èƒŒæ™¯ä¸å½±å“æ•…äº‹              |

---

å¦‚æœä½ æœ‰å…´è¶£æ·±å…¥é˜…è¯»å…¶ä¸­æŸç¯‡è®ºæ–‡æˆ–æƒ³è¦è·å–ä»£ç å®ç°ã€ä¸­æ–‡ç¿»è¯‘ç‰ˆæœ¬ç­‰èµ„æºï¼Œè¯·å‘Šè¯‰æˆ‘ï¼Œæˆ‘å¯ä»¥ç»§ç»­è¡¥å……ï¼

[1]: https://arxiv.org/abs/1706.03762?utm_source=chatgpt.com "Attention Is All You Need"
[2]: https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf?utm_source=chatgpt.com "[PDF] Attention is All you Need - NIPS"
[3]: https://zh.wikipedia.org/wiki/%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%B0%B1%E6%98%AF%E4%BD%A0%E6%89%80%E9%9C%80%E8%A6%81%E7%9A%84%E4%B8%80%E5%88%87?utm_source=chatgpt.com "æ³¨æ„åŠ›å°±æ˜¯ä½ æ‰€éœ€è¦çš„ä¸€åˆ‡"
[4]: https://blog.csdn.net/DeliaPu/article/details/136527834?utm_source=chatgpt.com "Transformerä¸­Self-Attentionçš„è¯¦ç»†è§£è¯» - CSDNåšå®¢"
[5]: https://www.reddit.com/r/deeplearning/comments/k5wn5k/resourcespapers_to_understand_transformers_and/?utm_source=chatgpt.com "Resources/papers to understand transformers and attention - Reddit"
[6]: https://arxiv.org/abs/2006.04768?utm_source=chatgpt.com "Linformer: Self-Attention with Linear Complexity"
[7]: https://arxiv.org/abs/2102.03902?utm_source=chatgpt.com "NystrÃ¶mformer: A NystrÃ¶m-Based Algorithm for Approximating Self-Attention"
[8]: https://arxiv.org/abs/1906.06755?utm_source=chatgpt.com "Theoretical Limitations of Self-Attention in Neural Sequence Models"
[9]: https://arxiv.org/abs/2310.20307?utm_source=chatgpt.com "Causal Interpretation of Self-Attention in Pre-Trained Transformers"
[10]: https://www.wired.com/story/eight-google-employees-invented-modern-ai-transformers-paper?utm_source=chatgpt.com "8 Google Employees Invented Modern AI. Here's the Inside Story"
[11]: https://www.techradar.com/pro/what-are-transformer-models?utm_source=chatgpt.com "What are transformer models?"
