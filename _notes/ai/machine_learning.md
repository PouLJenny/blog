# 机器学习


要预测的是离散值的，则为分类（classification）。
要预测的是连续值的，则为回归（regression）。

学得模型适用于新样本的能力，称为“泛化（generalization）”能力


## 奥卡姆剃刀(Occam's Razor)

是一种哲学原则，用来指导我们在面对多个解释时如何做出选择。它的核心思想是：
> 如无必要，勿增实体
> Entia non sunt multiplicanda sine necessitate.
简单来说就是： 在多个假设都能解释现象的情况下，最简单的那个通常是最好的。

举例说明：
假设你早上醒来，发现草地是湿的。
可能的解释包括：
1. 昨天晚上下雨了
2. 有人昨天晚上偷偷给你草地浇了水，然后走了。

两个解释都有可能，但根据奥卡姆剃刀原则，我们倾向于选择更简单的那个解释： 下雨了。


但其实有些情况下，我们是不太好区分那个解释更简单的，需要借助其他的机制才行。


## 数学知识

机器学习的灵魂是概率统计，骨架是线性代数，血肉是优化，心脏是微积分。


### 1. **线性代数**

机器学习几乎所有模型都依赖矩阵运算。

* 向量与矩阵运算
* 矩阵分解（特征值、特征向量、SVD、PCA）
* 范数与距离（L1、L2、余弦相似度）
* 正交与投影
  👉 应用：线性回归、PCA、神经网络中的权重矩阵



### 2. **概率论与数理统计**

理解不确定性、建模数据分布的核心。

* 随机变量与分布（伯努利、二项分布、正态分布、指数分布）
* 条件概率、贝叶斯公式
* 联合分布、边缘分布
* 数学期望、方差、协方差
* 极大似然估计 (MLE)、最大后验估计 (MAP)
* 中心极限定理、大数定律
  👉 应用：朴素贝叶斯、隐马尔可夫模型、生成模型、贝叶斯网络



### 3. **数值优化**

几乎所有机器学习算法 = 优化问题。

* 凸函数与凸优化
* 梯度下降（GD, SGD, Mini-batch）
* 牛顿法、拟牛顿法
* 拉格朗日乘子法、KKT 条件
* 正则化（L1, L2）
  👉 应用：逻辑回归、SVM、神经网络训练



### 4. **微积分**

描述变化与优化的基础。

* 函数的极限、连续、可导
* 偏导数、梯度、Hessian矩阵
* 链式法则（反向传播的核心）
* Taylor 展开
👉 应用：神经网络反向传播、优化算法



### 5. **统计学 & 推断**

让模型不仅拟合，还能泛化。

* 参数估计（点估计、区间估计）
* 假设检验、p 值
* 方差分析 (ANOVA)
* 信息论（熵、KL 散度、互信息）
* 偏差-方差分解
👉 应用：模型评估、正则化、信息增益（决策树）



### 6. **离散数学（基础）**

* 集合与逻辑
* 图论（图神经网络 GNN, 马尔可夫链）
* 组合数学（可能性计数、排列组合）
👉 应用：图模型、搜索算法、网络结构



### 7. **数值分析**

确保算法能在计算机上跑得快又稳。

* 浮点数运算与误差
* 数值稳定性（如 softmax 中的 log-sum-exp 技巧）
* 矩阵分解与近似计算
👉 应用：深度学习训练中的梯度消失/爆炸处理



### 📊 总结（机器学习数学地图）

* **线性代数** → 数据的表示与变换
* **概率与统计** → 不确定性建模
* **微积分** → 模型优化与训练
* **优化理论** → 找到最优参数
* **信息论** → 特征选择与模型评估
* **数值分析** → 计算稳定性
* **离散数学/图论** → 特殊模型（图模型、组合优化）

# EOF