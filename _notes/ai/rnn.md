# RNN

Recurrent Neural Network


## 相关论文

Elman, Jeffrey L. (1990).
Finding structure in time.
这篇论文最早提出了简单的循环神经网络（Elman RNN）。

Hochreiter, Sepp, and Jürgen Schmidhuber (1997).
Long Short-Term Memory.
经典的LSTM论文，解决了普通RNN的梯度消失问题，奠定了长序列建模的基础。

Cho, Kyunghyun et al. (2014).
Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.
介绍了GRU（Gated Recurrent Unit）模型，也是RNN在机器翻译领域的里程碑论文

Graves, Alex et al. (2013).
Speech Recognition with Deep Recurrent Neural Networks.
把RNN应用到语音识别领域，展示了深层RNN的强大表达能力。

Sutskever, Ilya et al. (2014).
Sequence to Sequence Learning with Neural Networks.
提出了Seq2Seq框架，奠定了后续机器翻译和对话系统等应用的基础。

Pascanu, Razvan et al. (2013).
On the difficulty of training Recurrent Neural Networks.
分析了RNN在训练时遇到的梯度爆炸/消失问题，并提出了相应的解决方法。

Bahdanau, Dzmitry et al. (2014).
Neural Machine Translation by Jointly Learning to Align and Translate.
虽然是基于Seq2Seq，但引入了Attention机制，极大地推动了NLP的发展。

Vaswani et al. (2017).
Attention is All You Need.
虽然它提出了Transformer，但可以视作RNN在NLP中的革命性替代方案。



