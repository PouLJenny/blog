# 文本挖掘方法

## 课程回放

密码： 8284

### 1
录制：250222上午《文本挖掘方法》信息学院在职课程班
录制文件：https://meeting.tencent.com/crm/Km3JaZ6v59

### 2
录制：250222下午《文本挖掘方法》信息学院在职课程班
录制文件：https://meeting.tencent.com/crm/2OD5P3Z928

### 3
录制：250223上午《文本挖掘方法》信息学院在职课程班
录制文件：https://meeting.tencent.com/crm/NXoLzkA8a4

### 4
录制：250223下午《文本挖掘方法》信息学院在职课程班
录制文件：https://meeting.tencent.com/crm/29D4rPRxfa


## Note

### 250222上午

- Deep Learning Models 
    - CNN、RNN & LSTM 、Self-attentions, Transformers
- Large Language Models
    - BERT、GTP、T5


#### Matlab
https://matlab.mathworks.com/

非常适合个人研究，代码比python还简洁，


课程作业：
https://openi.pcl.ac.cn/info-ruc/tm25projects



### 250222下午

#### 深度模型CNN 

卷积层，全连接层，pooling层


### 250223上午

- RNN 
- LSTM  

- Self-attentions, Transformers
- Large Language Models
    - BERT（encoder only）
    - GPT (decoder only)
    - T5 (both encoder and decoder)

训练模型不需要人工标注，直接随机的挖空，然后让大模型做完形填空就可以了


### 250223下午

- [openi](https://openi.pcl.ac.cn/)

# EOF
